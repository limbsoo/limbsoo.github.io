---
categories:
  - 기타
tags:
  - ML
---
# 정리
___


퍼셉트론(Perceptron)
: 다수의 신호를 입력받아, 하나의 신호를 출력(뉴런과 비슷한 구조)

※ AND, OR, NAND는 단층 퍼셉트론으로 표현할 수 있으나 XOR 같은 경우는 불가능, 단층 퍼셉트론은 직선형 영역만 표현할 수 있고, 다층 퍼셉트론은 비선형 영역도 표현


※ 신경망은 크게 입력층, 은닉층, 출력층 이렇게 3개로 구분

활성화 함수 (activation function)
: 입력 신호의 총합을 출력 신호로 변환하는 함수
	ex) 계단함수, 시그모이드 함수, ReLU함수

※ 일반적으로 회귀에서는 항등 함수, 분류에서는 소프트맥스 함수를 사용

배치(Batch)
: 입력 데이터를 묶은 것을 의미함 (즉, 하나로 묶은 데이터)

데이터 주도 학습 (기계학습)
- 사람이 개입하는 것이 아니다
- 데이터를 훈련 데이터와 시험 데이터로 나눠 학습과 실험을 수행
- 훈련 데이터만 사용하여 학습하면서 최적의 매개변수를 찾는다.
- 시험 데이터를 사용하여 앞서 훈련한 모델의 실력을 평가한다.

=> 아직 보지 못한 데이터로도 문제를 올바르게 풀어내는지 테스트 하기 위함이다.(범용 능력)

손실 함수
- 신경망 성능의 "나쁨"을 나타내는 지표
- 신경망 학습에서 현재의 상태를 '하나의 지표'로 표현하는데, 그 지표가 바로 손실 함수이다. 손실 함수는 가중치 매개변수의 값의 탐색에 따라서 달라진다. 손실 함수는 임의의 함수를 사용할 수 도 있지만, 일반적으로 평균 제곱 오차와 교차 엔트로피 오차를 사용

미니배치 학습
: 훈련 데이터로부터 일부만 골라 학습을 수행하는 것을 미니배치(mini-batch)라고 한다.

미분
: 다른 말로 순간변화율

편미분
: 변수가 두 개 이상일 때, 하나를 상수 취급하고 미분하는 것을 편미분이라고 한다. 

기울기
: x0, x1의 편미분을 동시에 계산하여 벡터로 정리한 것을 기울기 (gradient)

경사법(경사 하강법)
- 최적의 매개변수를 찾아야한다. 즉 손실 함수가 최솟값이 될 때의 매개변수 값을 찾아야하는데, 기울기를 잘 이용해 함수의 최솟값을 찾으려고 하는 것이 경사법이다.
- 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한다. 그 후에 다시 기울기를 구해, 기울어진 방향으로 나아가기를 반복한다. 이렇게 함수의 값을 점차 줄이는 것이 경사법(gradient method)

하이퍼 파리미터
: 매개변수, 가중치 X  / 학습률(보폭), 시행착오

정확도 
: 훈련데이터로 평가

오차역전파법 
: 신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산

연쇄법칙 
: 합성 함수(여러 함수로 구성된 함수)의 미분에 대한 성질

※ 신경망 학습은 다음과 같이 4단계(미니배치, 기울기 산출, 매개변수 갱신, 반복)

에폭(Epoch)
: 1 epoch는 학습 과정에서 학습 데이터를 모두 소진했을 때의 횟수

매개변수 갱신
: 신경망 학습의 목적은 '손실 함수의 값을 최대한 낮추는 매개변수를 찾는 것'이다. 이를 위해서 매개변수의 기울기를 이용하였고, 갱신한 매개변수 값으로 다시 갱신하고, 갱신하는 방법을 취했다. 이를 SGD, 확률적 경사 하강법

모멘텀
: 기본적으로 SGD와 유사하지만, "속도(v)"라는 개념이 추가됨

AdaGrad
: 신경망 학습에서 중요한 학습률(learning rate)을 서서히 낮추는 방법임
즉, 학습률 감소가 매개변수의 원소마다 다르게 적용됨

Adam
: Momentum과 AdaGrad 방법을 결합한 방법임

※  가중치의 초깃값 설정이 매우 중요, 그렇다고 가중치의 초깃값을 "0"으로 설정하면 안 된다 고르게 분포X

ReLU: He 초기값
sigmoid나 tanh등 S자 모양 곡선:  Xavier 초기값

배치 정규화(Batch Normalization)
학습 시 미니배치를 단위로 정규화를 수행
학습을 빨리 진행할 수 있다.
초기값에 크게 의존하지 않음
오버피팅을 억제함 (dropout 등의 필요성 감소

오버피팅(Overfitting) 
기계학습에서는 학습 데이터에 대한 정확도는 높으나 테스트 데이터에 대한 정확도는 떨어지는 과적합문제가 발생하는 경우

가중치 감소(Weight Decay)
과적합은 가중치 매개변수의 값이 커서 발생하는 경우가 많기 때문에, 학습 과정에서 큰 가중치에 대해서는 그에 상응하는 큰 페널티를 부과하여 과적합을 억제하는 방법임

드롭아웃(Dropout)
뉴런을 임의로 삭제하면서 학습하는 방법

하이퍼파라미터
사용자가 주는 옵션값
- 각층의 뉴런 수, 배치 크기, 매개변수 갱신 시 학습률, 가중치 감소
- 적절한 설정이 이루어지지 않으면 모델의 성능이 크게 떨어짐

완전연결 계층의 문제점
Affine 계층의 문제점은 "데이터의 형상(shape)이 무시된다 (형상에 담긴 공간적 정보를 무시)

합성곱 연산
이미지 처리에서 말하는 필터 연산

패딩
패딩: 합성곱 연산을 수행하기 전에 입력 데이터 주변을 특정 값(ex 0)으로 채우는 기법

스트라이드
필터를 적용하는 위치의 간격

풀링(Pooliing)
세로 * 가로 방향의 공간을 줄이는 연산으로, 일종의 subsampling이 주된 목적이라고 할 수 있겠다.(즉, 이미지의 크기를 축소하는 것이 목표라는 의미이다)
최대 풀링(Max Pooling)"과 "평균 풀링(Average Pooling)

데이터 확장
입력 이미지(학습 이미지)를 알고리즘을 동원하여 인위적으로 확장하는 것

층을 깊게 하는 것이 왜 중요한지에 대해 설명
적은 매개변수로 같은(혹은 그 이상) 수준의 표현력을 달성함
학습 데이터의 양을 줄여서 고속 학습이 가능함 (즉, 학습의 효율성이 높아짐)
정보를 계층적으로 전달할 수 있음

VGG
합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN임
비중 있는 층(합성곱 계층, 완전연결 계층)을 모두 16층(혹은 19층)으로 심화한 것이 특징임

GoogLeNet
세로 방향 깊이 뿐만 아니라 가로 방향도 깊다는 점이 특징임

ResNet
딥러닝 학습에서는 층이 지나치게 깊으면 학습이 잘 되지 않고 오히려 성능이 떨어지는데, ResNet은 이런 문제를 "스킵 연결"로 해결함
스킵 연결이란 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조를 뜻함

전이 학습(Transfer Learning)
학습된 가중치(혹은 그 일부)를 다른 신경망에 복사한 다음, 그 상태로 재학습(fine tuning)을 수행

딥러닝 프레워크는 GPU를 활용하여 대량의 연산을 고속으로 처리

딥러닝은 사물 인식 뿐만 아니라 사물 검출과 분할에도 이용할 수 있으며, 응용 분야로는 사진의 캡션 생성, 이미지 생성, 강화학습 등이 있다.

DQN
딥러닝과 강화학습 결합 인간수준 높은 성능

자연어 처리(natural language processing): 컴퓨터가 사람 말을 이해하도록 하는 것

시소러스
유의어 집합 + 그래프, 유의어사전 

통계 기반 기법

말뭉치(corpus)
자연어 처리 연구나 애플리케이션을 염두에 두고 수집된 텍스트 데이터, 문장들은 자연어의 특성 말뭉치에서 자동적, 효율적으로 단어의 의미들을 추출

단어의 분산 표현(distributional representation)
단어의 의미를 벡터로 표현, 분포 가설(단어의 의미는 주변 단어에 의해 형성된다)에 기반

동시 발생 행렬(co-occurrence matrix)
어떤 단어에 대해서 윈도우 크기에 따른 주변에 동시 발생한 단어들이 어떤 것들이 있고 몇 번 등장했는지를 집계하여 행렬로 나타낸 것, 각 행은 해당 단어를 표현한 벡터가 된다

단어 벡터 간 유사도 
두 벡터 x, y가 가리키는 방향이 얼마나 비슷한지

점별 상호정보량(poinwise mutual information)
단지 발생 횟수만으로 관련성을 판단하기에는 관사같이 고빈도 단어들같은 예외가 있음
PMI 척도: 값이 높을수록 관련성도 높다

차원 감소(dimensionality reduction)
중요한 정보는 최대한 유지하면서 벡터의 차원 줄이기

통계 기반 기법, 추론 기반 기법
말뭉치 전체의 통계를 이용해 1회의 처리로 단어의 분산 표현 획득
말뭉치가 대규모 크기일 경우 행렬의 크기도 대규모가 되고 여기에 O(n^3) 비용을 갖는 SVD를 적용해서 차원 감소를 하는 것도 비효율적

추론 기반 기법은 미니배치 학습

신경망(모델)에서의 단어 처리
- 단어 -> 원핫 벡터

CBOW 모델 개요
- 맥락으로부터 타깃을 추론

skip-gram 모델
CBOW는 주변 단어들을 통해서 중간의 단어를 예측하는 모델이였다면 Skip-Gram은 중심 단어를 통해 주변단어를 예측하는 모델이다. 

word2vec 속도 개선
앞 장에서 구현한 간단한 word2vec의 CBOW 모델의 문제점: 말뭉치 크기가 많이 크다면 그에 따라 계산량도 굉장히 많아진다 

Embedding 계층 도입
- 기존의 원핫 표현과 MatMul 계층의 가중치 곱은 결국 가중치 행렬의 특정 행을 추출하는 것

Negative Sampling 손실 함수 도입 
부정적 예시

CBOW 모델 평가
벡터 공간에서 man -> woman 벡터와 king -> ? 벡터가 가능한 한 가까워지는 단어 찾기

유추 문제(단어의 의미나 문법적인 문제를 제대로 이해하고 있는지)

단어 벡터 평가 방법
- 궁극적으로 원하는 건 정확도 높은 시스템(여러 시스템으로 구성, ex: 단어의 분산 표현 만드는 시스템 + 특정 문제에 대해 분류를 수행하는 시스템)
- 단어의 분산 표현의 차원 수가 시스템에 어떤 영향을 주는지?
