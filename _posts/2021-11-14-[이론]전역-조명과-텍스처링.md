---
categories:
  - 컴퓨터 그래픽스
tags:
  - OpenGL
  - 컴퓨터그래픽스
  - 이론
---

# 1. 조명 모델 
___
<br>
## 1. 지역 조명(local illumination)

조명 대상인 물체의 표면 재질과 광원의 속성만 이용해 해당 표면의 색상을 결정할 뿐, 같은 공간에 있는 다른 물체는 고려하지 않는다.(퐁 모델)

퐁 모델로 $S_2$를 라이팅할 때, $S_1$은 전혀 고려되지 않는다. 따라서, 그림에서 확인할 수 있듯이, 마치 $S_1$이 존재하지 않는 것처럼 $S_2$는 빛을 받는다. 퐁 모델은 물리학적으로 올바르지 않은 모델이다.

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/8b5cc0e8-36a6-4354-be55-4d68e906b942" alt width="90%">
<em>지역 조명</em>
</center>



## 2. 전역 조명(global illumination) 

### 1) 광선추적법(ray tracing)

뷰 프러스텀은 카메라에 수렴하는 투영선(projection line)의 집합으로, 투영선의 개수는 스크린 영상의 해상도와 같아, 하나의 투영선에 하나의 픽셀이 대응되어 픽셀의 색상을 결정한다. 

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/42fc3731-2083-430f-8b57-4729ee0fc08f" alt width="50%">
<em>지역 조명</em>
</center>


1차 광선(primary ray) : 투영선 역방향으로 발사하는 광선으로, 이를 추적하여 해당 투영선에 대응하는 픽셀의 색상을 계산한다.

2차 광선(secondary ray) : 1차 광선이 어떤 물체와 부딪히면 그 교차점에서 각 광원을 향해 발사하는 광선으로, 그림자 여부를 판단하는 그림자 광선(shadow ray)이라고도 한다.


<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/65e1fa35-90cd-42ff-a942-15e7e14b7b58" alt width="80%">
<em>지역 조명</em>
</center>


1차 광선은  $p_1$에서 교차했는데, 여기에서 그림자 광선 $s_1$이 광원을 향해 발사되었다.
만약 $s_1$이 광원으로 가는 도중 다른 물체와 부딪히면 $p_1$은 광원의 직접적인 영향권에 있지 않으며 그림자 영역에 놓인 것으로 판정한다.
반면, $s_1$이 광원에 도달하게 되면, 이 광원으로부터 $p_1$에 입사하는 빛을 이용해 $p_1$의 직접 조명 색상을 결정한다. 이를 위해서 퐁 모델을 쓸 수 있다.

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/e8096fcb-69a1-49d2-acb2-250383cbc046" alt width="50%">
<em>지역 조명</em>
</center>

한편, 그림자 광선 말고도 두 가지 2차 광선이 $p_1$으로부터 추가로 발사된다. 하나는 반사 광선(reflection ray)인데, 이는 1차 광선이 $p_1$의 노멀 $n_1$을 중심으로 반사된 것이다.
1ck rhkdtjsdms $I_1$으로, 반사 광선은 $r_1$으로 표기하자. $I_1$의 입사각과 $r_1$의 반사각이 같다는 성질을 이용하여, $r_1$의 방향은 다음과 같이 구해진다.
<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/9019bcec-78b2-4e7f-8e5e-64bd7de6a6bc" alt width="40%">
<em>지역 조명</em>
</center>

또 하나의 2차 광선은 굴절 광선(refraction ray 혹은 transmitted ray)인데, 이는 1차 광선이 반투명한 물체와 부딪힐 때 발생한다.
굴절 광선을 $t_1$으로 표기했는데, $t_1$의 방향은 굴절의 법칙에 따라 정해진다. (반투명할 때)

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/1f276ae1-24bd-4c94-af4e-776f551bb874" alt width="40%">
<em>지역 조명</em>
</center>



광선 추적법은 재귀적인(recursive) 알고리즘이다. 즉, (b)에서 $r_1$과 $t_1$은 마치 이들이 1차 광선인 것처럼 다시 추적된다.



광선 추적법 알고리즘의 재귀적인 구조는 (c)의 광선 트리(ray tree)로 설명할 수 있다. 이 광선 트리는 반사/굴절 광선이 어떤 물체에도 부딪히지 않고 장면을 벗어날 때까지 또는 미리 정의된 재귀 단계에 도달할 때까지 확장된다.
$p_3$를 생각해보면, $r_3$를 추적해서 얻은 색상은 $p_3$의 스페큘러 계수와 곱해지고, $t_3$를 추적해서 얻은 색상은 $p_3$의 투과율(transmission coefficient)과 곱해진다. 이 둘은 $s_3$를 통해서 계산된 직접 조명 색상에 더해지고, 그 결과는 부모 노드인 $p_1$으로 전달된다.
광선 트리의 모든 노드에서 이와 같은 작업이 재귀적으로 진행되면 최종적으로 루트 노드에서의 색상을 얻을 수 있다.

# 2. 환경 매핑
___

다음 그림과 같이 주변 환경을 반사하는 매끄러운 물체를 렌더링하는 기법이 환경 매핑(environment mapping)이다.

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/fb6510ed-14bd-42c9-b554-c7ff6fb154c7" alt width="80%">
<em>지역 조명</em>
</center>



이를 위해 첫 번째로 해야 할 일은 환경맵(environment map)이라고 불리는 텍스처에 주변 환경의 영상을 담는 것이다.

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/b4c226a1-9341-4e91-8054-416d05846df3" alt width="80%">
<em>지역 조명</em>
</center>


가장 많이 쓰이는 환경맵은 큐브맵(cube map)이다.
이 정육면체의 여섯 개 면 각각을 향해서 수직과 수평 방향으로 모두 90˚의 시야각(field of view)을 설정해 사진을 찍거나 렌더링을 수행하면 여섯 개의 정사각형 이미지를 얻을 수 있다.


<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/7a4d5602-836e-4e6f-94fc-887450eccdd1" alt width="80%">
<em>지역 조명</em>
</center>

물체 표면의 점 p가 주변 환경을 반사하도록 만들기 위해서, 카메라로부터 p를 향해 광선 $I$를 발사하고 이를 추적한다. $I$는 p의 표면 노멀 n을 기준으로 반사되어 $R$이라는 벡터를 결정한다.

이는 전역 조명을 위한 광선 추적법에서 소개된 식과 동일하다. $R$은 큐브맵의 한 면과 교차하는데, 이 교차점을 이용해 해당 면의 이미지 텍스처를 필터링한다.
예를 들면, 교차점 주변의 네 개 텍셀을 겹선형보간하는 것이다. 이 결과가 바로 p에서 반사되는 색상을 결정한다.
이러한 큐브 매핑은 간략화된 광선 추적법이라 부를 수 있는데, 2차 광선 중 반사 광선만 추적이 되고 재귀적인 과정도 거치지 않기 때문


환경 매핑 구현 방법은 매우 간단하지만 그 결과는 훌륭하다. 지역 조명 모델로 렌더링된 영상에 더해져서 전역 조명 모델의 분위기를 만들어준다.
하지만, 지역 조명 모델의 근본적 한계를 극복하지 못한다. 예를 들어, 물체는 자기 자신을 반사하지 못한다. 주전자 그림에서 보듯이 주전자 손잡이가 주전자 몸체에 반사되지 않는 모습을 볼 수 있다.






































퐁 모델은 지역 조명 모델인데, 이는 조명 대상인 물체의 표면 재질과 광원의 속성만 이용해 해당 표면의 색상을 결정할 뿐, 같은 공간에 있는 다른 물체는 고려하지 않는다.

퐁 모델로 $S_2$를 라이팅할 때, $S_1$은 전혀 고려되지 않는다. 따라서, 그림에서 확인할 수 있듯이, 마치 $S_1$이 존재하지 않는 것처럼 $S_2$는 빛을 받는다. 퐁 모델은 물리학적으로 올바르지 않은 모델이다.


컴퓨터 그래픽스 역사 초기에 제안된 전역 조명 기법은 광선 추적법(ray tracing)과 래디오 시티(radiosity)인데, 이들은 현재에도 여전히 널리 사용되고 있다.


뷰 프러스텀은 카메라에 수렴하는 투영선(projection line)의 집합임을 상기하자. 투영선의 개수는 스크린 영상의 해상도와 같고, 투영선 하나가 픽셀 하나의 색상을 결정한다.


광선 추적법 알고리즘에서는 각 투영선 반대 방향으로 광선(ray)을 발사한뒤 이를 추적하여 해당 투영선을 따라 들어오게 될 색상을 계산한다. 이것이 바로 픽셀의 색상이 된다.
이렇게 투영선 역방향으로 발사되는 광선을 1차 광선(primary ray)이라 부른다.

1차 광선이 어떤 물체와 부딪히면 우선 그 교차점이 그림자 안에 있는가를 검사한다. 이를 위해 그림자 광선(shadow ray)이라는 이름의 2차 광선(secondary ray)을 각 광원을 향해 발사한다.
 

(b)의 경우 1차 광선은 커다란 구와 $p_1$에서 교차했는데, 여기에서 그림자 광선 $s_1$이 광원을 향해 발사되었다.
만약 $s_1$이 광원으로 가는 도중 다른 물체와 부딪히면 $p_1$은 광원의 직접적인 영향권에 있지 않으며 그림자 영역에 놓인 것으로 판정한다.
반면, $s_1$이 광원에 도달하게 되면, 이 광원으로부터 $p_1$에 입사하는 빛을 이용해 $p_1$의 직접 조명 색상을 결정한다. 이를 위해서 퐁 모델을 쓸 수 있다.
 

한편, 그림자 광선 말고도 두 가지 2차 광선이 $p_1$으로부터 추가로 발사된다. 하나는 반사 광선(reflection ray)인데, 이는 1차 광선이 $p_1$의 노멀 $n_1$을 중심으로 반사된 것이다.
1ck rhkdtjsdms $I_1$으로, 반사 광선은 $r_1$으로 표기하자. $I_1$의 입사각과 $r_1$의 반사각이 같다는 성질을 이용하여, $r_1$의 방향은 다음과 같이 구해진다.


또 하나의 2차 광선은 굴절 광선(refraction ray 혹은 transmitted ray)인데, 이는 1차 광선이 반투명한 물체와 부딪힐 때 발생한다.
굴절 광선을 $t_1$으로 표기했는데, $t_1$의 방향은 굴절의 법칙에 따라 정해진다. (반투명할 때)

광선 추적법은 재귀적인(recursive) 알고리즘이다. 즉, (b)에서 $r_1$과 $t_1$은 마치 이들이 1차 광선인 것처럼 다시 추적된다.
 

광선 추적법 알고리즘의 재귀적인 구조는 (c)의 광선 트리(ray tree)로 설명할 수 있다. 이 광선 트리는 반사/굴절 광선이 어떤 물체에도 부딪히지 않고 장면을 벗어날 때까지 또는 미리 정의된 재귀 단계에 도달할 때까지 확장된다.
$p_3$를 생각해보면, $r_3$를 추적해서 얻은 색상은 $p_3$의 스페큘러 계수와 곱해지고, $t_3$를 추적해서 얻은 색상은 $p_3$의 투과율(transmission coefficient)과 곱해진다. 이 둘은 $s_3$를 통해서 계산된 직접 조명 색상에 더해지고, 그 결과는 부모 노드인 $p_1$으로 전달된다.
광선 트리의 모든 노드에서 이와 같은 작업이 재귀적으로 진행되면 최종적으로 루트 노드에서의 색상을 얻을 수 있다.



다음 그림과 같이 주변 환경을 반사하는 매끄러운 물체를 렌더링하는 기법이 환경 매핑(environment mapping)이다.
이를 위해 첫 번째로 해야 할 일은 환경맵(environment map)이라고 불리는 텍스처에 주변 환경의 영상을 담는 것이다.

가장 많이 쓰이는 환경맵은 큐브맵(cube map)이다.
이 정육면체의 여섯 개 면 각각을 향해서 수직과 수평 방향으로 모두 90˚의 시야각(field of view)을 설정해 사진을 찍거나 렌더링을 수행하면 여섯 개의 정사각형 이미지를 얻을 수 있다.

물체 표면의 점 p가 주변 환경을 반사하도록 만들기 위해서, 카메라로부터 p를 향해 광선 $I$를 발사하고 이를 추적한다. $I$는 p의 표면 노멀 n을 기준으로 반사되어 $R$이라는 벡터를 결정한다.

이는 전역 조명을 위한 광선 추적법에서 소개된 식과 동일하다. $R$은 큐브맵의 한 면과 교차하는데, 이 교차점을 이용해 해당 면의 이미지 텍스처를 필터링한다.
예를 들면, 교차점 주변의 네 개 텍셀을 겹선형보간하는 것이다. 이 결과가 바로 p에서 반사되는 색상을 결정한다.
이러한 큐브 매핑은 간략화된 광선 추적법이라 부를 수 있는데, 2차 광선 중 반사 광선만 추적이 되고 재귀적인 과정도 거치지 않기 때문


환경 매핑 구현 방법은 매우 간단하지만 그 결과는 훌륭하다. 지역 조명 모델로 렌더링된 영상에 더해져서 전역 조명 모델의 분위기를 만들어준다.
하지만, 지역 조명 모델의 근본적 한계를 극복하지 못한다. 예를 들어, 물체는 자기 자신을 반사하지 못한다. 주전자 그림에서 보듯이 주전자 손잡이가 주전자 몸체에 반사되지 않는 모습을 볼 수 있다.









































쉐도우 매핑 알고리즘은 두 번의 렌더링 패스(rendering pass)를 통해 수행된다.
첫 번째 패스에서는 쉐도우맵(shadow map)이라는 특수한 텍스처를 생성한다. (a)는 광원에서 나온 빛이 미치는 표면은 굵은 선으로 표시되었다.
이들 표면을 균일하게 샘플하여, 각 샘플점 $p$마다 광원까지의 거리를 쉐도우 맵에 저장한다.
이 거리를 $z$로 표기하는데, 이는 광원에서 본 3차원 장면의 깊이다. (Z Buffer)
따라서, 쉐도우맵은 광원 기준의 깊이맵(depth map)이라고도 한다.
 

두 번째 패스에서는 실제 렌더링을 수행하는데, 이 과정에서 쉐도우맵을 사용하여 그림자를 생성한다.
(b)에서 프래그먼트 $f_1$을 처리하는 경우를 보면, 이 프래그먼트에 해당하는 월드 공간의 점을 $q_1$이라 하면, 광원과 $q_1$ 사이의 거리 $d_1$은 쉐도우맵에 저장된 깊이값 $z_1$보다 크다.
$z_1$이라는 깊이 값을 가진 어떤 점이 광원과 $q_1$사이에 놓여 있어서 $q_1$으로 향하는 빛을 가리고 있다는 것을 의미한다. 따라서 $q_1$은 그림자에 속하는 점으로 판정된다.
반면, 광원까지의 거리 $d_2$는 쉐도우맵에 저장된 값 $z_2$와 같은 $q_2$는 광원에서 보이는 점이 되고, 따라서 $q_2$는 빛을 받는 점으로 판정된다.


(b)는 쉐도우 매핑을 사용해 장면을 렌더링한 결과를 보여준다. 완전하게 빛을 받는 지역에 자잘한 그림자가 섞여 있다.


쉐도우맵은 텍스처의 한 종류이므로 이를 어떻게 필터링할 것인지 미리 정해야 하는데, 근접점 샘플링(nearest point sampling)을 사용한다고 가정하자. 그러면 $q_1$의 경우, 쉐도우맵에서 $z_1$을 읽어올 것이다.
이는 $q_1$과 광원 사이의 거리 $d_1$보다 작기 때문에 $q_1$은 그림자에 속하는 점으로 판정되고, 이는 잘못된 판정이다.
이처럼 완전하게 빛을 받는 지역임에도 일부 프래그먼트는 빛을 못 받는 것으로 판정되어 (b)처럼 자잘한 그림자가 생성되는 것이다.



이 문제를 해결하는 방법은 간단하다. 두 번째 패스에서 샘플한 점들을 광원 쪽으로 약간 이동하면 된다.
즉 광원까지의 거리 $d$에서 일정한 값을 빼는 것이다. 이 값을 바이어스(bias)라 한다.
(d)에서는 $d_1$에서 바이어스를 빼서 $d_1'$를 만든 후 이르 $z_1$과 비교하였다. $d_1'$가 $z_1$보다 작으므로 $q_1$은 빛을 받는 것으로 판정된다. 이러한 방법을 통해 쉐도우 매핑을 개선한 결과를 보여준다.


쉐도우 매핑에서는 적절한 바이어스를 설정하는 것이 중요한데, 바이어스가 너무 작을 경우 불필요한 자잘한 그림자를 완벽히 제거할 수 없다.
바이어스는 대개 몇 번의 시험을 거쳐 적절한 값으로 설정된다.


(a)의 월드 공간 점 $q_1$과 $q_2$를 보면, 이는 쉐도우맵으로 투영(project)되어 두 텍셀 $p_1$과 $p_2$ 사이에 놓이게 된다.
근접점 샘플링으로 쉐도우맵을 필터링하면, $q_1$은 $p_1$과 비교되어 그림자 지는 것으로 판정되고, $q_2$는 $p_2$와 비교되어 빛을 받는 것으로 판정된다.
즉, 하나의 프래그먼트는 완전히 빛을 받거나 완전히 그늘지거나 둘 중 하나로 판정될 뿐, 다른 여지는 없어진다.
그 결과 (b)와 같이 거친 윤곽선을 가진 그림자가 생성된다.

이제 겹선형보간으로 쉐도우맵을 필터링해 보자.
(c)는 쉐도우맵에 투영된 프래그먼트 $q$를 보여준다. $q$를 둘러싼 네 개 텍셀의 깊이 값은 쉐도우맵에서 읽어온 것이고, 이 깊이 값을 겹선형보간하면 64가 된다.
만약, $q$의 깊이가 80이라면, $q$는 그늘진 것으로 판정된다. 즉, 완전히 빛을 받거나 완전히 그늘지거나 둘 중 하나로 판정되는 문제는 여전하다. 겹선형보간이 문제를 해결해 주지 않는다.
 

이 문제를 해결하는 방법은 네 개의 텍셀 각각에 대해 $q$의 가시성(visibility)을 결정한 후 이를 보간하는 것이다.
(d)를 보면, 왼쪽 위의 텍셀만 고려하면 $q$에는 그림자가 맺히는 것으로 판정될 것이다. 즉, 가시성은 0이다.
반면, 나머지 세 개의 텍셀에 대해서는 $q$는 빛을 받는 것으로 판정된다. 즉, 가시성은 1이다.
네 개의 가시성 값을 겹선형보간하면 $q$의 가시성은 0.58로 계산된다. 이 값은 $q$가 빛을 받는 정도를 나타낸다.
완전히 빛을 받거나 완전히 그늘지거나 둘 중 하나를 택하는 대신, [0, 1] 범위 안의 값을 취하게 된다. 0에 가까우면 어둡게, 1에 가까우면 밝게 처리된다.
(d)의 그림자 윤곽선은 (c)보다 부드러워졌다.
 

쉐도우맵에서 여러 개의 텍셀을 참조하여 이들에 대한 픽셀의 가시성을 결정하고 그 결과를 결합하는 기법을 일반적으로 PCF(percentage closer filtering)라고 부른다.










(d)에서 세 개의 점 a, b, c에서의 디퓨즈 반사를 보면 이는 노멀 $n$과 빛 벡터 $l$에 의해 결정되는데, b의 경우 $n$과 $l$ 사이 각도가 작아서 빛을 많이 받아 밝게 보인다.
하지만, a와 c는 $n$과 $l$ 사이 각도가 커서 빛을 적게 받고 어두워 보인다.
 

표면 전체에 걸쳐 이처럼 노멀이 불규칙하게 변하므로, 표면 밝기도 불규칙하게 변해서 오돌토돌한 표면을 잘 표현해 준다.
하지만 폴리곤 메시의 해상도가 너무 높은 고해상도 메시는 처리에 많은 시간이 소요된다.


빠른 연산을 위해 삼각형 두 개로 구성된 메시를 사용하였지만, 이런 메시로는 오돌토돌한 표면을 잘 표현할 수 없다.
(d)를 보면 a에서 b를 거쳐 c로 갈 때, $n$과 $l$ 사이의 각도는 서서히 작아져서 점차 밝아보이게 된다.
서서히 변화하는 표면 밝기로 인해서 표면의 오돌토돌한 특징이 드러나지 않게 된다.
 

이러한 문제를 해결하기 위한 방법 중에 하나가 바로 고해상도 메시의 노멀을 미리 계산하고, 이를 노멀맵(normal map)이라고 하는 특수한 텍스처에 저장한 후, 런타임에는 그림 (a)의 저해상도 메시를 처리하되 노멀맵으로부터 노멀을 읽어서 이를 라이팅에 사용하는 것이다.
이러한 텍스처링 기법을 노멀 매핑(normal mapping) 혹은 범프 매핑(bump mapping)이라 부른다.


왼쪽 표면은 이른바 하이트 필드(height field)로 표현할 수 있다.
2차원 좌표(x, y)가 주어졌을 때 높이(height) 혹은 z값을 반환하는 함수 h(x,y)이다. 왼쪽 그림은 일정한 간격의 (x, y) 좌표에서 샘플된 하이트 필드를 보여준다.
이러한 높이 값을 저장한 2차원 텍스처를 하이트맵(height map)이라고 한다.
 

높이 값을 회색조(gray-scale) 색상으로 해석하면 하이트맵은 회색조 이미지로 그릴 수 있다.
색상이 비교적 일정한 부분은 상대적으로 매끈한 영역을 나타내고, 색상이 불규칙한 부분은 오돌토돌한 영역을 나타낸다.

 (a)의 이미지 텍스처가 주어졌다면, 각 픽셀의 RGB 값을 회색조로 바꾸고, 필요에 따라 이를 수작업으로 편집하여 (b)의 하이트맵을 얻을 수 있다.
이렇게 생성된 하이트맵은 원본 텍스처와 동일한 해상도를 가진다.


노멀맵을 만드는 방법 중 하나는 하이트맵을 사용하는 것이다.
(a)는 아까의 하이트맵을 다시 그린것이고, 하이트맵 텍셀 9개를 이용해 사각형 메시를 만들었다. 이 메시 표면의 가운데 점 (x, y, h(x, y))의 노멀을 계산해보자.
 

이렇게 계산된 노멀은 하이트 필드의 바깥쪽을 가리키며 (x, y, h(x, y)) 주변의 표면 경사도를 반영한다.
(c)는 각 텍셀에 노멀이 저장된 노멀맵의 개념을 보여준다. 일반적으로 노멀맵은 하이트맵과 동일한 해상도를 가진다.
 

정규화된 노멀 $(n_x, n_y, n_z)$의 각 좌표는 모두 [-1, 1] 범위의 실수값이다.
반면, 이런 노멀을 저장할 텍스처의 RGB 채널은 모두 [0, 1] 범위에 있다. 따라서 다음과 같은 범위 전환이 필요하다.
 

(c)에서 보인 것 처럼, 노멀맵은 (0, 0, 1)을 이쪽저쪽으로 조금씩 '흔들어 놓은' 노멀들의 집합으로 이해할 수 있다.
따라서, $n_z$가 $n_x$와 $n_y$에 비해 상대적으로 클 것이고, 계산된 R, G, B 중 B값이 크게 된다. 그렇게 되면 결국 노멀맵을 이미지 텍스처로 취급하여 그리면 전체적으로 파란 색조를 띄게 될 것이다.







GL은 컬러 버퍼(color buffer), 깊이 버퍼(depth buffer), 스텐실 버퍼(stencil buffer)를 제공하는데, 이들을 합해서 프레임 버퍼(frame buffer)라고 한다.
컬러 버퍼는 스크린의 2차원 뷰포트에 나타날 픽셀 전체를 저장하는 메모리 공간이다. (w x z 해상도)
깊이 버퍼는 z-버퍼라고도 불리는데, 컬러 버퍼와 동일한 해상도를 가지며 현재 컬러 버퍼에 저장되어 있는 픽셀의 z값을 저장한다.
z가 프래그먼트마다 겉으로 안들어나지만 계산이 된다. 프래그먼트에서 나오는 normal은 라이팅(lighting), texcoord는 텍스쳐링(texturing)에 사용된다.



스크린 공간 3차원 뷰포트의 z범위가 [0.0, 1.0]이라고 가정하자. 그러면 z-버퍼는 배경의 z값에 해당하는 1.0으로 초기화된다. 한편, 컬러 버퍼는 배경색(흰색)으로 초기화된다.
프래그먼트 쉐이더가 한 프래그먼트의 RGBA 색상을 출력할 때, 그 프래그먼트의 스크린 공간 좌표 (x, y, z)는 자동으로 출력 병합기에 전달된다.
그러면 출력 병합기는 그 z좌표를 현재 z-버퍼의 (x, y)에 기록되어 있는 z값과 비교한다. 만약 프래그먼트의 z좌표가 작다면, 이는 현재 컬러 버퍼에 저장되어 있는 픽셀보다 앞에 있다는 뜻이므로, 컬러 버퍼의 픽셀을 프래그먼트 색상으로 대체한다.
 

그림을 보면 (38, 56), (38, 57), (39, 56) 좌표에 위치한 픽셀들이 빨간색에서 파란색으로 바뀐 것에 주목하라. 동시에 z-버퍼의 같은 곳에서 z값 역시 0.8에서 0.5로 갱신되었다.
이처럼 z버퍼를 이용하여 픽셀과 프래그먼트의 앞뒤 순서를 가리는 알고리즘을 z-버퍼링(z-buffering) 혹은 깊이 버퍼링(depth buffering)이라 부른다.
원칙적으로 z-버퍼링 알고리즘은 삼각형들을 임의의 순서로 처리하는 것을 허용한다.



이 절은 프래그먼트와 픽셀이 혼합(blending)되어야 하는 경우를 설명할 것이다.
반투명(translucent) 물체를 렌더링할 때는 이야기가 달라진다. 반투명한 프래그먼트의 z좌표가 컬러 버퍼에 있는 픽셀의 z값보다 작을 경우, 픽셀의 색상은 프래그먼트를 통해 비쳐 보여야 한다.
이는 픽셀과 프래그먼트 색상의 혼합을 필요로 한다.
 

프래그먼트 쉐이더가 출력하는 프래그먼트의 RGBA 색상 중 A를 알파 채널(alpha channel)이라고도 하는데, 픽셀과의 혼합 과정은 프래그먼트의 알파 채널을 이용하므로 알파 블렌딩(alpha blending)이라 불린다.
A는 정수 범위보다는 정규화된 범위인 [0, 1]이 선호된다. 최소값 0은 '완전한 투명'을 나타내고 최대값 1은 '완전한 불투명'을 나타낸다.



여기서 c는 블렌딩된 색상을, α는 프래그먼트의 불투명도(알파 채널), c_f는 프래그먼트 색상, c_p는 픽셀 색상을 나타낸다.



파란색 삼각형은 반투명하고, 빨간색 삼각형은 불투명하다.
파란색 프래그먼트는 알파 블렌딩 식에 의해 빨간색 픽셀과 블렌딩된다.
 

하지만, 렌더링 순서가 바뀌어, 파란색 삼각형을 먼저, 빨간색 삼각형을 나중에 그리면 빨간색 프래그먼트의 z좌표 0.8은 파란색 픽셀의 z값 0.5보다 크므로 폐기될 것이다. 블렌딩이 이뤄지지 않는다.
 

z-버퍼링은 삼각형 처리 순서에 독립적이지만 이는 불투명한 삼각형들에만 적용된다. 반투명한 삼각형들은 이렇게 임의의 순서로 렌더링될 수 없다.
모든 불투명한 삼각형이 처리된 뒤, 반투명한 삼각형들은 뒤에서부터 앞으로(back-to-front) 차례차례 처리되어야 한다. 이를 위해 반투명 삼각형들은 정렬(sorting)되어야 한다.
 





라이팅(lighting or illumination)은 빛과 물체 간 상호작용 처리

퐁모델 라이팅을 위해 광원(light source) 정의 필요
광원의 종류: 점 광원(point light source) 3차원의 한 점으로부터 전방위로 빛이 발산
방향성 광원(directional light source) 물체 표면의 여러 점에 입사하는 빛의 빛의 방향이 서로 평행, 광원의 색상과 입사방향 고려

물체 표면에서 감지되는 색상을 디퓨즈(diffuse), 스페큘러(specular), 앰비언트(ambient),발산광(emissive light)


디퓨즈 : 난반사

표면의 빛은 모든 방향을 따라 같은 강도로 반사
카메라에 의해 감지되는 빛은 카메라의 시선에 무관하고 물체 표면의 빛의 방향에 비례
빛의 방향은 빛벡터(light vector) 로 정의


물체 표면의 점 p의 노멀(noraml) n과 빛 벡터 l사이의 각도 ∂를 입사각(incident angle)이라고 하는데, ∂가 작을수록 p는 더 많은 빛을 받는다.
n과 l의 내적을 사용하여 p에 들어오는 빛의 양을 결정할 수 있다.
cos∂는 점 p가 받는 빛의 양을 결정하게 된다.



위 식은 n · l이 음수가 되는 경우 0을 선택하여 p에 들어오는 빛의 양을 0으로 만든다.
위 식은 점 p에 들어오는 빛의 '강도'만을 결정한다.

여기서 s_d는 광원의 RGB 색상을, m_d는 물체의 디퓨즈 계수(diffuse reflectance)를 나타낸다.
(표기법에서 s는 광원(light source)을, m은 재질(material)을 의미한다.)


퐁 모델의 스페큘러 항은 물체 표면에 하이라이트(hightlight)를 만드는 데 사용된다.
이를 위해 시선 벡터(view vector)와 반사 벡터(reflection vector)가 필요하다.
시선 벡터는 물체 표면의 점 p와 카메라를 연결하는 것으로 v로 표기된다. 빛 벡터 l과 마찬가지로 v 역시 실제 카메라 시선과 반대 방향으로 정의된다.


p에 들어온 빛이 입사각 ∂와 동일한 각도를 이루며 반사되는 것을 정반사라고 하는데, 이 방향을 따른 벡터가 반사 벡터이며 r로 표기한다.
n과 l이 이루는 입사각 ∂는 n과 r이 이루는 반사각과 같다.두 직각삼각형은벡터 ncos∂로표현된 변을공유한다.왼쪽삼각형의 l과 ncos∂를 연결하는 벡터를 s로 표기하자
l이 단위 벡터이므로 r 역시 단위 벡터가 된다.

디퓨즈(난반사)는 카메라가 어느 위치에 있든 상관이 없지만, 스페큘러(정반사)는 카메라가 어느 위치에 있는지 중요하다.
하이라이트를 볼 수 있는 영역은 그림과 같이 r을 중심으로 한 원뿔 모양으로 묘사할 수 있다.

여기에서 sh는 shininess의 앞 글자로 표면의 매끈함의 정도를 나타낸다.
r과 v가 같으면 sh값에 관계없이 1이 되고 최대의 하이라이트가 카메라에 보인다. 하지만 r과 v가 다르면 sh가 커질수록 하이라이트는 급격히 감소하게 된다.

여기서 s_s는 광원의 색상이고, m_s는 물체의 스페큘러 계수(specular reflectance)이다. 디퓨즈와 마찬가지로 max함수가 필요하다.
m_s는 m_d와 달리 (0.9, 0.9, 0.9), (0.8, 0.8, 0.8) 등과 같은 회색조(gray-scale)로 표현되는데, 이는 물체 표면의 하이라이트가 광원의 색을 반영하도록 하기 위함이다.
→ 예를 들어 빨간색의 금속성 물체에 흰색 광원이 비치는 경우를 상상해 보자. 물체 표면이 빨간색이더라도 하이라이트 영역은 흰색으로 빛날 것이다.
따라서 m_s는 회색조로 설정되어, s_s가 반사되는 정도를 조절하게 된다.


공간 내 다양한 물체로부터 반사된 빛을 앰비언트 빛(ambient light)이라 부르는데, 이는 간접 조명에 해당한다.
앰비언트 빛은 특정한 방향이 아닌 '모든 방향을 따라' p점에 들어온다. 따라서 이는 p에서 '모든 방향을 따라' 반사된다.
결국 p에 들어오는 앰비언트 빛의 양은 p의 노멀에 무관하고, p에서 반사되는 빛의 양은 카메라 시선에 무관하다.

여기에서 s_a는 앰비언트 빛의 RGB 색상이고, m_a는 물체의 앰비언트 계수(ambient reflectance)이다.
간접 조명을 표현하는 앰비언트 항 덕분에 우리는 광원으로부터의 빛이 직접 닿지 않는 부분에도 조명 효과를 줄 수 있게 된다.

주전자의 다른 부분과 마찬가지로 약간의 조명을 받는다. 하지만 실세계 간접 조명의 디테일을 잡아내기에는 너무 단순화되어 있다.
16.4절에서 보다 세련된 앰비언트 반사 기법을 배울 것이다.

퐁 모델의 발산광 항은 물체 자신이 빛을 발산하는 경우에 사용된다.
퐁 모델은 발산광을 가진 물체를 광원으로 취급하지 않는다.
따라서 같은 공간의 다른 물체를 라이팅하는 데 기여하지 못한다. 이것은 퐁 모델의 명백한 한계 중 하나이다.

퐁 모델은 이상의 네 개 항을 더하여 다음과 같이 정의된다.
빛을 발산하지 않는 물체의 경우, 발산광 m_e를 삭제하면 된다.
만약 물체가 램버시안 표면에 가깝다면 m_d를 크게, m_s를 작게 설정한다.
반면, 반짝이는 금속성 물체를 표현하기 위해서는 m_s를 크게 설정한다.



<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/c9145e65-b6f2-48ca-a3ff-70e15bc1358e" alt width="50%">
<em>2차원에서의 변환 결합 </em>
</center>

```c++
bool Renderer::isBackFace(int nFace)
{
	float normalZ = 0;
	Vector4 v1, v2, v3;
	v1.x = m_tramsformedVertex[m_face[nFace].m_vertex[0] - 1][0];
	v1.y = m_tramsformedVertex[m_face[nFace].m_vertex[0] - 1][1];
	v2.x = m_tramsformedVertex[m_face[nFace].m_vertex[1] - 1][0];
	v2.y = m_tramsformedVertex[m_face[nFace].m_vertex[1] - 1][1];
	v3.x = m_tramsformedVertex[m_face[nFace].m_vertex[2] - 1][0];
	v3.y = m_tramsformedVertex[m_face[nFace].m_vertex[2] - 1][1];
	normalZ = ((v2.x - v1.x) * (v3.y - v1.y)) - ((v2.y - v1.y) * (v3.x - v1.x));

	if (normalZ < 0) return true;
	return false;
}

```

```c++
g_renderer.readTextureFile("speckled213.raw");
g_renderer.makeUVVertex();
g_renderer.makefaceNormals();
g_renderer.makeVertexNormals();

void Renderer::readTextureFile(char* pFileName)
{
	FILE* input_file;

	char input_data[checkImageHeight][checkImageWidth][3];

	input_file = fopen(pFileName, "rb");

	fread(input_data, sizeof(char), checkImageWidth * checkImageHeight * 3, input_file);

	fclose(input_file);

	for (int i = 0; i < checkImageHeight; i++)
	{
		for (int j = 0; j < checkImageWidth; j++)
		{
			m_texture[i][j][0] = (GLubyte)input_data[i][j][0];
			m_texture[i][j][1] = (GLubyte)input_data[i][j][1];
			m_texture[i][j][2] = (GLubyte)input_data[i][j][2];
		}
	}
}

void Renderer::makeVertexUVs()
{
	for (int i = 0; i < m_nNumVertex; i++)
	{
		Vector4 v(m_vertex[i], 1);
		m_uv[i][0] = (v.x + 1) * 320;
		m_uv[i][1] = (v.y + 1) * 240;
	}
}

void Renderer ::makefaceNormals() //외적 
{
	for (int i = 0; i < m_nNumFace; i++) // 각 face의 normal
	{
		Vector4 v1, v2;

		v1.x = m_vertex[m_face[i].m_vertex[0] - 1][0] - m_vertex[m_face[i].m_vertex[1] - 1][0];
		v1.y = m_vertex[m_face[i].m_vertex[0] - 1][1] - m_vertex[m_face[i].m_vertex[1] - 1][1];

		v1.z = m_vertex[m_face[i].m_vertex[0] - 1][2] - m_vertex[m_face[i].m_vertex[1] - 1][2];

		v2.x = m_vertex[m_face[i].m_vertex[1] - 1][0] - m_vertex[m_face[i].m_vertex[2] - 1][0];
		v2.y = m_vertex[m_face[i].m_vertex[1] - 1][1] - m_vertex[m_face[i].m_vertex[2] - 1][1];

		v2.z = m_vertex[m_face[i].m_vertex[1] - 1][2] - m_vertex[m_face[i].m_vertex[2] - 1][2];

		Vector4 ret;

		ret = crossProduct(v1, v2);
		ret = normalize(ret);

		m_faceNormal[i][0] = ret.x;
		m_faceNormal[i][1] = ret.y;
		m_faceNormal[i][2] = ret.z;
	}

}

void Renderer::makeVertexNormals()
{
	for (int i = 0; i < m_nNumFace; i++) // //점의 노말은 점이 속한 삼각형 노말의 평균
	{
		for (int j = 0; j < 3; j++)
		{  
			m_vertexNormal[m_face[i].m_vertex[j] - 1][0] += m_faceNormal[i][0];
			m_vertexNormal[m_face[i].m_vertex[j] - 1][1] += m_faceNormal[i][1];
			m_vertexNormal[m_face[i].m_vertex[j] - 1][2] += m_faceNormal[i][2];
		}
	}

	for (int i = 0; i < m_nNumVertex; i++)
	{
		Vector4 v;

		v.x = m_vertexNormal[i][0];
		v.y = m_vertexNormal[i][1];
		v.z = m_vertexNormal[i][2];
		
		v = normalize(v);

		m_vertexNormal[i][0] = v.x;
		m_vertexNormal[i][1] = v.y;
		m_vertexNormal[i][2] = v.z;
	}

}
```




```c++
void Renderer::render()
{
	if (!texureMappingEnabled)
	{
		clearCheckImage();
		clearZBuffer();
		Matrix4 mvp = m_proj * m_view * m_world; //m_world 부터 역순으로 vector에 곱함
		applyMatrix(mvp, m_world);
		clearEdgetable();

		for (int i = 0; i < m_nNumFace; i++)
		{
			clearEdgetable();
			if (isCullEnabled)
			{
				if (!isBackFace(i))
				{
					buildEdgetable(i);
					fill(m_face[i].m_color);
				}
			}
			else
			{
				buildEdgetable(i);
				fill(m_face[i].m_color);
			}
		}
	}
}

void Renderer::buildEdgetable(int nFace)
{
	float vertices[2][3];
	int ymin;
	float yMax, x, inverseOfSlope, z, zPerY;
	
	for (int i = 0; i < m_face[nFace].m_nNumVertex; i++)
	{
		for (int j = 0; j < 3; j++)
		{
			vertices[0][j] = m_tramsformedVertex[m_face[nFace].m_vertex[i] - 1][j];
			vertices[1][j] = m_tramsformedVertex[m_face[nFace].m_vertex[(i + 1) % m_face[nFace].m_nNumVertex] - 1][j];
		}

		float uvVertices[2][2];
		float u, v, uPerY, vPerY;
		for (int j = 0; j < 2; j++)
		{
			uvVertices[0][j] = m_uv[m_face[nFace].m_vertex[i] - 1][j];
			uvVertices[1][j] = m_uv[m_face[nFace].m_vertex[(i + 1) % m_face[nFace].m_nNumVertex] - 1][j];
		}

		if (vertices[0][1] == vertices[1][1]) continue;
		else
		{
			inverseOfSlope = (vertices[1][0] - vertices[0][0]) / (vertices[1][1] - vertices[0][1]); //xperY
		}
		
		zPerY = (vertices[1][2] - vertices[0][2]) / (vertices[1][1] - vertices[0][1]);
		uPerY = (uvVertices[1][0] - uvVertices[0][0]) / (vertices[1][1] - vertices[0][1]); 
		vPerY = (uvVertices[1][1] - uvVertices[0][1]) / (vertices[1][1] - vertices[0][1]);

		float savedY;
		float savedV;

		if (vertices[0][1] < vertices[1][1]) // 작을때 ceiling 클 때 floor 
		{
			savedY = vertices[0][1];
			ymin = ceil(vertices[0][1]);
			ymin = max(ymin, 0);

			if (ymin > checkImageHeight - 1) continue;

			m_ET[ymin][m_indexCount[ymin]].x = vertices[0][0];

			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].x += (ymin - savedY) * inverseOfSlope;
			}

			m_ET[ymin][m_indexCount[ymin]].zperY = zPerY;
			m_ET[ymin][m_indexCount[ymin]].yMax = vertices[1][1];
			m_ET[ymin][m_indexCount[ymin]].inverseOfSlope = inverseOfSlope;
			m_ET[ymin][m_indexCount[ymin]].z = vertices[1][2];

			if (savedY != 0) v = ymin * uvVertices[0][1] / savedY;
			else v = 0;

			m_ET[ymin][m_indexCount[ymin]].u = uvVertices[0][0];
			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].u += (ymin - savedY) * uPerY;
			}
			m_ET[ymin][m_indexCount[ymin]].v = v;
			m_ET[ymin][m_indexCount[ymin]].uPerY = uPerY;
			m_ET[ymin][m_indexCount[ymin]].vperY = vPerY;
			m_indexCount[ymin]++;
		}
		else
		{
			savedY = vertices[1][1];
			ymin = ceil(vertices[1][1]);
			ymin = max(ymin, 0);

			if (ymin > checkImageHeight - 1) continue;
			m_ET[ymin][m_indexCount[ymin]].x = vertices[1][0];
			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].x += (ymin - savedY) * inverseOfSlope;
			}
			m_ET[ymin][m_indexCount[ymin]].yMax = vertices[0][1];  
			m_ET[ymin][m_indexCount[ymin]].inverseOfSlope = inverseOfSlope;
			m_ET[ymin][m_indexCount[ymin]].z = vertices[0][2];
			savedV = uvVertices[1][1];
			if (savedY != 0)
			{
				v = ymin * uvVertices[1][1] / savedY;
			}
			else v = 0;
			m_ET[ymin][m_indexCount[ymin]].u = uvVertices[1][0];
			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].u += (ymin - savedY) * uPerY;
			}
			m_ET[ymin][m_indexCount[ymin]].v = v;
			m_ET[ymin][m_indexCount[ymin]].uPerY = uPerY;
			m_ET[ymin][m_indexCount[ymin]].vperY = vPerY;
			m_indexCount[ymin]++;
		}
	}
}

void Renderer::fill(GLubyte color[3])
{
	// AET
	for (int i = 0; i < checkImageHeight; i++)
	{

		//update intersection
		for (int j = 0; j < m_numEdgeInAET; j++)
		{
			m_AET[j].x += m_AET[j].inverseOfSlope;
			m_AET[j].z += m_AET[j].zperY;
			m_AET[j].u += m_AET[j].uPerY;
			m_AET[j].v += m_AET[j].vperY;
		}

		//Add new edge
		for (int j = 0; j < m_indexCount[i]; j++)
		{
			m_AET[m_numEdgeInAET + j] = m_ET[i][j];
		}
		m_numEdgeInAET += m_indexCount[i];

		//Delete edge
		for (int j = 0; j < m_numEdgeInAET; j++)
		{
			if (m_AET[j].yMax < i)
			{
				for (int k = j; k < m_numEdgeInAET; k++)
				{
					m_AET[k] = m_AET[k + 1];
				}
				j--;
				m_numEdgeInAET--;
			}
		}

		//Sort intersections
		Edge temp;
		for (int j = 0; j < m_numEdgeInAET - 1; j++)
		{
			for (int k = j + 1; k < m_numEdgeInAET; k++)
			{
				if (m_AET[j].x > m_AET[k].x)
				{
					temp = m_AET[j];
					m_AET[j] = m_AET[k];
					m_AET[k] = temp;
				}
			}
		}

		//Render
		for (int j = 0; j < m_numEdgeInAET; j += 2)
		{
			int k;
			int xmin = floor(m_AET[j].x);
			int xmax = floor(m_AET[j + 1].x);
			xmin = max(xmin, 0);
			xmax = min(xmax, checkImageWidth - 1);
			float uPerX = (m_AET[j + 1].u - m_AET[j].u) / (xmax - xmin);
			float deltaX = 0;
			float vPerX = (m_AET[j + 1].v - m_AET[j].v) / (xmax - xmin);
			float deltaV = 0;
			float zPerX = (m_AET[j + 1].z - m_AET[j].z) / (xmax - xmin);
			float deltaZ = 0;

			for (k = xmin; k < xmax; k++)
			{
				if (m_AET[j].z + deltaZ < zBuffer[i][k])
				{
					checkImage[i][k][0] = (GLubyte)texture[(int)(m_AET[j].v + deltaV)][(int)(m_AET[j].u + deltaX)][0];
					checkImage[i][k][1] = (GLubyte)texture[(int)(m_AET[j].v + deltaV)][(int)(m_AET[j].u + deltaX)][1];
					checkImage[i][k][2] = (GLubyte)texture[(int)(m_AET[j].v + deltaV)][(int)(m_AET[j].u + deltaX)][2];

					zBuffer[i][k] = m_AET[j].z + deltaZ;
					deltaX += uPerX;
					deltaV += vPerX;
					deltaZ += zPerX;
				}
			}
		}
	}
}

```




<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/c9145e65-b6f2-48ca-a3ff-70e15bc1358e" alt width="50%">
<em>2차원에서의 변환 결합 </em>
</center>