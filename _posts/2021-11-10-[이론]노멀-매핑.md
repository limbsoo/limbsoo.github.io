---
categories:
  - 컴퓨터 그래픽스
tags:
  - OpenGL
  - 컴퓨터그래픽스
---

# 1. 6322342
<br>

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/7780a4b6-c211-4baf-a40a-149ec58f91d4" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>

(d)에서 세 개의 점 a, b, c에서의 디퓨즈 반사를 보면 이는 노멀 $n$과 빛 벡터 $l$에 의해 결정되는데, b의 경우 $n$과 $l$ 사이 각도가 작아서 빛을 많이 받아 밝게 보인다.
하지만, a와 c는 $n$과 $l$ 사이 각도가 커서 빛을 적게 받고 어두워 보인다.

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/47505e50-fcfb-4d40-92ef-a3f42a7222ca" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>

표면 전체에 걸쳐 이처럼 노멀이 불규칙하게 변하므로, 표면 밝기도 불규칙하게 변해서 오돌토돌한 표면을 잘 표현해 준다.
하지만 폴리곤 메시의 해상도가 너무 높은 고해상도 메시는 처리에 많은 시간이 소요된다.


빠른 연산을 위해 삼각형 두 개로 구성된 메시를 사용하였지만, 이런 메시로는 오돌토돌한 표면을 잘 표현할 수 없다.
(d)를 보면 a에서 b를 거쳐 c로 갈 때, $n$과 $l$ 사이의 각도는 서서히 작아져서 점차 밝아보이게 된다.
서서히 변화하는 표면 밝기로 인해서 표면의 오돌토돌한 특징이 드러나지 않게 된다.
 
<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/508114fe-42b0-4de6-9ec3-29d036500e48" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>


이러한 문제를 해결하기 위한 방법 중에 하나가 바로 고해상도 메시의 노멀을 미리 계산하고, 이를 노멀맵(normal map)이라고 하는 특수한 텍스처에 저장한 후, 런타임에는 그림 (a)의 저해상도 메시를 처리하되 노멀맵으로부터 노멀을 읽어서 이를 라이팅에 사용하는 것이다.
이러한 텍스처링 기법을 노멀 매핑(normal mapping) 혹은 범프 매핑(bump mapping)이라 부른다.
<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/d1564121-9dda-49f8-819a-8c295f334dd0" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>


왼쪽 표면은 이른바 하이트 필드(height field)로 표현할 수 있다.
2차원 좌표(x, y)가 주어졌을 때 높이(height) 혹은 z값을 반환하는 함수 h(x,y)이다. 왼쪽 그림은 일정한 간격의 (x, y) 좌표에서 샘플된 하이트 필드를 보여준다.
이러한 높이 값을 저장한 2차원 텍스처를 하이트맵(height map)이라고 한다.
 

높이 값을 회색조(gray-scale) 색상으로 해석하면 하이트맵은 회색조 이미지로 그릴 수 있다.
색상이 비교적 일정한 부분은 상대적으로 매끈한 영역을 나타내고, 색상이 불규칙한 부분은 오돌토돌한 영역을 나타낸다.

 (a)의 이미지 텍스처가 주어졌다면, 각 픽셀의 RGB 값을 회색조로 바꾸고, 필요에 따라 이를 수작업으로 편집하여 (b)의 하이트맵을 얻을 수 있다.
이렇게 생성된 하이트맵은 원본 텍스처와 동일한 해상도를 가진다.


노멀맵을 만드는 방법 중 하나는 하이트맵을 사용하는 것이다.
(a)는 아까의 하이트맵을 다시 그린것이고, 하이트맵 텍셀 9개를 이용해 사각형 메시를 만들었다. 이 메시 표면의 가운데 점 (x, y, h(x, y))의 노멀을 계산해보자.
 

이렇게 계산된 노멀은 하이트 필드의 바깥쪽을 가리키며 (x, y, h(x, y)) 주변의 표면 경사도를 반영한다.
(c)는 각 텍셀에 노멀이 저장된 노멀맵의 개념을 보여준다. 일반적으로 노멀맵은 하이트맵과 동일한 해상도를 가진다.
 

정규화된 노멀 $(n_x, n_y, n_z)$의 각 좌표는 모두 [-1, 1] 범위의 실수값이다.
반면, 이런 노멀을 저장할 텍스처의 RGB 채널은 모두 [0, 1] 범위에 있다. 따라서 다음과 같은 범위 전환이 필요하다.
 

(c)에서 보인 것 처럼, 노멀맵은 (0, 0, 1)을 이쪽저쪽으로 조금씩 '흔들어 놓은' 노멀들의 집합으로 이해할 수 있다.
따라서, $n_z$가 $n_x$와 $n_y$에 비해 상대적으로 클 것이고, 계산된 R, G, B 중 B값이 크게 된다. 그렇게 되면 결국 노멀맵을 이미지 텍스처로 취급하여 그리면 전체적으로 파란 색조를 띄게 될 것이다.


<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/538c65c9-c605-4c05-bfca-eee5487122ae" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>




<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/d5b8ef34-0321-4780-996e-97e8171500a2" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>

<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/79e46981-b4e3-4ca7-a978-8a6d2453878e
" alt width="50%">
<em>1 Pass, 깊이맵 생성</em>
</center>





































(d)에서 세 개의 점 a, b, c에서의 디퓨즈 반사를 보면 이는 노멀 $n$과 빛 벡터 $l$에 의해 결정되는데, b의 경우 $n$과 $l$ 사이 각도가 작아서 빛을 많이 받아 밝게 보인다.
하지만, a와 c는 $n$과 $l$ 사이 각도가 커서 빛을 적게 받고 어두워 보인다.
 

표면 전체에 걸쳐 이처럼 노멀이 불규칙하게 변하므로, 표면 밝기도 불규칙하게 변해서 오돌토돌한 표면을 잘 표현해 준다.
하지만 폴리곤 메시의 해상도가 너무 높은 고해상도 메시는 처리에 많은 시간이 소요된다.


빠른 연산을 위해 삼각형 두 개로 구성된 메시를 사용하였지만, 이런 메시로는 오돌토돌한 표면을 잘 표현할 수 없다.
(d)를 보면 a에서 b를 거쳐 c로 갈 때, $n$과 $l$ 사이의 각도는 서서히 작아져서 점차 밝아보이게 된다.
서서히 변화하는 표면 밝기로 인해서 표면의 오돌토돌한 특징이 드러나지 않게 된다.
 

이러한 문제를 해결하기 위한 방법 중에 하나가 바로 고해상도 메시의 노멀을 미리 계산하고, 이를 노멀맵(normal map)이라고 하는 특수한 텍스처에 저장한 후, 런타임에는 그림 (a)의 저해상도 메시를 처리하되 노멀맵으로부터 노멀을 읽어서 이를 라이팅에 사용하는 것이다.
이러한 텍스처링 기법을 노멀 매핑(normal mapping) 혹은 범프 매핑(bump mapping)이라 부른다.


왼쪽 표면은 이른바 하이트 필드(height field)로 표현할 수 있다.
2차원 좌표(x, y)가 주어졌을 때 높이(height) 혹은 z값을 반환하는 함수 h(x,y)이다. 왼쪽 그림은 일정한 간격의 (x, y) 좌표에서 샘플된 하이트 필드를 보여준다.
이러한 높이 값을 저장한 2차원 텍스처를 하이트맵(height map)이라고 한다.
 

높이 값을 회색조(gray-scale) 색상으로 해석하면 하이트맵은 회색조 이미지로 그릴 수 있다.
색상이 비교적 일정한 부분은 상대적으로 매끈한 영역을 나타내고, 색상이 불규칙한 부분은 오돌토돌한 영역을 나타낸다.

 (a)의 이미지 텍스처가 주어졌다면, 각 픽셀의 RGB 값을 회색조로 바꾸고, 필요에 따라 이를 수작업으로 편집하여 (b)의 하이트맵을 얻을 수 있다.
이렇게 생성된 하이트맵은 원본 텍스처와 동일한 해상도를 가진다.


노멀맵을 만드는 방법 중 하나는 하이트맵을 사용하는 것이다.
(a)는 아까의 하이트맵을 다시 그린것이고, 하이트맵 텍셀 9개를 이용해 사각형 메시를 만들었다. 이 메시 표면의 가운데 점 (x, y, h(x, y))의 노멀을 계산해보자.
 

이렇게 계산된 노멀은 하이트 필드의 바깥쪽을 가리키며 (x, y, h(x, y)) 주변의 표면 경사도를 반영한다.
(c)는 각 텍셀에 노멀이 저장된 노멀맵의 개념을 보여준다. 일반적으로 노멀맵은 하이트맵과 동일한 해상도를 가진다.
 

정규화된 노멀 $(n_x, n_y, n_z)$의 각 좌표는 모두 [-1, 1] 범위의 실수값이다.
반면, 이런 노멀을 저장할 텍스처의 RGB 채널은 모두 [0, 1] 범위에 있다. 따라서 다음과 같은 범위 전환이 필요하다.
 

(c)에서 보인 것 처럼, 노멀맵은 (0, 0, 1)을 이쪽저쪽으로 조금씩 '흔들어 놓은' 노멀들의 집합으로 이해할 수 있다.
따라서, $n_z$가 $n_x$와 $n_y$에 비해 상대적으로 클 것이고, 계산된 R, G, B 중 B값이 크게 된다. 그렇게 되면 결국 노멀맵을 이미지 텍스처로 취급하여 그리면 전체적으로 파란 색조를 띄게 될 것이다.







GL은 컬러 버퍼(color buffer), 깊이 버퍼(depth buffer), 스텐실 버퍼(stencil buffer)를 제공하는데, 이들을 합해서 프레임 버퍼(frame buffer)라고 한다.
컬러 버퍼는 스크린의 2차원 뷰포트에 나타날 픽셀 전체를 저장하는 메모리 공간이다. (w x z 해상도)
깊이 버퍼는 z-버퍼라고도 불리는데, 컬러 버퍼와 동일한 해상도를 가지며 현재 컬러 버퍼에 저장되어 있는 픽셀의 z값을 저장한다.
z가 프래그먼트마다 겉으로 안들어나지만 계산이 된다. 프래그먼트에서 나오는 normal은 라이팅(lighting), texcoord는 텍스쳐링(texturing)에 사용된다.



스크린 공간 3차원 뷰포트의 z범위가 [0.0, 1.0]이라고 가정하자. 그러면 z-버퍼는 배경의 z값에 해당하는 1.0으로 초기화된다. 한편, 컬러 버퍼는 배경색(흰색)으로 초기화된다.
프래그먼트 쉐이더가 한 프래그먼트의 RGBA 색상을 출력할 때, 그 프래그먼트의 스크린 공간 좌표 (x, y, z)는 자동으로 출력 병합기에 전달된다.
그러면 출력 병합기는 그 z좌표를 현재 z-버퍼의 (x, y)에 기록되어 있는 z값과 비교한다. 만약 프래그먼트의 z좌표가 작다면, 이는 현재 컬러 버퍼에 저장되어 있는 픽셀보다 앞에 있다는 뜻이므로, 컬러 버퍼의 픽셀을 프래그먼트 색상으로 대체한다.
 

그림을 보면 (38, 56), (38, 57), (39, 56) 좌표에 위치한 픽셀들이 빨간색에서 파란색으로 바뀐 것에 주목하라. 동시에 z-버퍼의 같은 곳에서 z값 역시 0.8에서 0.5로 갱신되었다.
이처럼 z버퍼를 이용하여 픽셀과 프래그먼트의 앞뒤 순서를 가리는 알고리즘을 z-버퍼링(z-buffering) 혹은 깊이 버퍼링(depth buffering)이라 부른다.
원칙적으로 z-버퍼링 알고리즘은 삼각형들을 임의의 순서로 처리하는 것을 허용한다.



이 절은 프래그먼트와 픽셀이 혼합(blending)되어야 하는 경우를 설명할 것이다.
반투명(translucent) 물체를 렌더링할 때는 이야기가 달라진다. 반투명한 프래그먼트의 z좌표가 컬러 버퍼에 있는 픽셀의 z값보다 작을 경우, 픽셀의 색상은 프래그먼트를 통해 비쳐 보여야 한다.
이는 픽셀과 프래그먼트 색상의 혼합을 필요로 한다.
 

프래그먼트 쉐이더가 출력하는 프래그먼트의 RGBA 색상 중 A를 알파 채널(alpha channel)이라고도 하는데, 픽셀과의 혼합 과정은 프래그먼트의 알파 채널을 이용하므로 알파 블렌딩(alpha blending)이라 불린다.
A는 정수 범위보다는 정규화된 범위인 [0, 1]이 선호된다. 최소값 0은 '완전한 투명'을 나타내고 최대값 1은 '완전한 불투명'을 나타낸다.



여기서 c는 블렌딩된 색상을, α는 프래그먼트의 불투명도(알파 채널), c_f는 프래그먼트 색상, c_p는 픽셀 색상을 나타낸다.



파란색 삼각형은 반투명하고, 빨간색 삼각형은 불투명하다.
파란색 프래그먼트는 알파 블렌딩 식에 의해 빨간색 픽셀과 블렌딩된다.
 

하지만, 렌더링 순서가 바뀌어, 파란색 삼각형을 먼저, 빨간색 삼각형을 나중에 그리면 빨간색 프래그먼트의 z좌표 0.8은 파란색 픽셀의 z값 0.5보다 크므로 폐기될 것이다. 블렌딩이 이뤄지지 않는다.
 

z-버퍼링은 삼각형 처리 순서에 독립적이지만 이는 불투명한 삼각형들에만 적용된다. 반투명한 삼각형들은 이렇게 임의의 순서로 렌더링될 수 없다.
모든 불투명한 삼각형이 처리된 뒤, 반투명한 삼각형들은 뒤에서부터 앞으로(back-to-front) 차례차례 처리되어야 한다. 이를 위해 반투명 삼각형들은 정렬(sorting)되어야 한다.
 





라이팅(lighting or illumination)은 빛과 물체 간 상호작용 처리

퐁모델 라이팅을 위해 광원(light source) 정의 필요
광원의 종류: 점 광원(point light source) 3차원의 한 점으로부터 전방위로 빛이 발산
방향성 광원(directional light source) 물체 표면의 여러 점에 입사하는 빛의 빛의 방향이 서로 평행, 광원의 색상과 입사방향 고려

물체 표면에서 감지되는 색상을 디퓨즈(diffuse), 스페큘러(specular), 앰비언트(ambient),발산광(emissive light)


디퓨즈 : 난반사

표면의 빛은 모든 방향을 따라 같은 강도로 반사
카메라에 의해 감지되는 빛은 카메라의 시선에 무관하고 물체 표면의 빛의 방향에 비례
빛의 방향은 빛벡터(light vector) 로 정의


물체 표면의 점 p의 노멀(noraml) n과 빛 벡터 l사이의 각도 ∂를 입사각(incident angle)이라고 하는데, ∂가 작을수록 p는 더 많은 빛을 받는다.
n과 l의 내적을 사용하여 p에 들어오는 빛의 양을 결정할 수 있다.
cos∂는 점 p가 받는 빛의 양을 결정하게 된다.



위 식은 n · l이 음수가 되는 경우 0을 선택하여 p에 들어오는 빛의 양을 0으로 만든다.
위 식은 점 p에 들어오는 빛의 '강도'만을 결정한다.

여기서 s_d는 광원의 RGB 색상을, m_d는 물체의 디퓨즈 계수(diffuse reflectance)를 나타낸다.
(표기법에서 s는 광원(light source)을, m은 재질(material)을 의미한다.)


퐁 모델의 스페큘러 항은 물체 표면에 하이라이트(hightlight)를 만드는 데 사용된다.
이를 위해 시선 벡터(view vector)와 반사 벡터(reflection vector)가 필요하다.
시선 벡터는 물체 표면의 점 p와 카메라를 연결하는 것으로 v로 표기된다. 빛 벡터 l과 마찬가지로 v 역시 실제 카메라 시선과 반대 방향으로 정의된다.


p에 들어온 빛이 입사각 ∂와 동일한 각도를 이루며 반사되는 것을 정반사라고 하는데, 이 방향을 따른 벡터가 반사 벡터이며 r로 표기한다.
n과 l이 이루는 입사각 ∂는 n과 r이 이루는 반사각과 같다.두 직각삼각형은벡터 ncos∂로표현된 변을공유한다.왼쪽삼각형의 l과 ncos∂를 연결하는 벡터를 s로 표기하자
l이 단위 벡터이므로 r 역시 단위 벡터가 된다.

디퓨즈(난반사)는 카메라가 어느 위치에 있든 상관이 없지만, 스페큘러(정반사)는 카메라가 어느 위치에 있는지 중요하다.
하이라이트를 볼 수 있는 영역은 그림과 같이 r을 중심으로 한 원뿔 모양으로 묘사할 수 있다.

여기에서 sh는 shininess의 앞 글자로 표면의 매끈함의 정도를 나타낸다.
r과 v가 같으면 sh값에 관계없이 1이 되고 최대의 하이라이트가 카메라에 보인다. 하지만 r과 v가 다르면 sh가 커질수록 하이라이트는 급격히 감소하게 된다.

여기서 s_s는 광원의 색상이고, m_s는 물체의 스페큘러 계수(specular reflectance)이다. 디퓨즈와 마찬가지로 max함수가 필요하다.
m_s는 m_d와 달리 (0.9, 0.9, 0.9), (0.8, 0.8, 0.8) 등과 같은 회색조(gray-scale)로 표현되는데, 이는 물체 표면의 하이라이트가 광원의 색을 반영하도록 하기 위함이다.
→ 예를 들어 빨간색의 금속성 물체에 흰색 광원이 비치는 경우를 상상해 보자. 물체 표면이 빨간색이더라도 하이라이트 영역은 흰색으로 빛날 것이다.
따라서 m_s는 회색조로 설정되어, s_s가 반사되는 정도를 조절하게 된다.


공간 내 다양한 물체로부터 반사된 빛을 앰비언트 빛(ambient light)이라 부르는데, 이는 간접 조명에 해당한다.
앰비언트 빛은 특정한 방향이 아닌 '모든 방향을 따라' p점에 들어온다. 따라서 이는 p에서 '모든 방향을 따라' 반사된다.
결국 p에 들어오는 앰비언트 빛의 양은 p의 노멀에 무관하고, p에서 반사되는 빛의 양은 카메라 시선에 무관하다.

여기에서 s_a는 앰비언트 빛의 RGB 색상이고, m_a는 물체의 앰비언트 계수(ambient reflectance)이다.
간접 조명을 표현하는 앰비언트 항 덕분에 우리는 광원으로부터의 빛이 직접 닿지 않는 부분에도 조명 효과를 줄 수 있게 된다.

주전자의 다른 부분과 마찬가지로 약간의 조명을 받는다. 하지만 실세계 간접 조명의 디테일을 잡아내기에는 너무 단순화되어 있다.
16.4절에서 보다 세련된 앰비언트 반사 기법을 배울 것이다.

퐁 모델의 발산광 항은 물체 자신이 빛을 발산하는 경우에 사용된다.
퐁 모델은 발산광을 가진 물체를 광원으로 취급하지 않는다.
따라서 같은 공간의 다른 물체를 라이팅하는 데 기여하지 못한다. 이것은 퐁 모델의 명백한 한계 중 하나이다.

퐁 모델은 이상의 네 개 항을 더하여 다음과 같이 정의된다.
빛을 발산하지 않는 물체의 경우, 발산광 m_e를 삭제하면 된다.
만약 물체가 램버시안 표면에 가깝다면 m_d를 크게, m_s를 작게 설정한다.
반면, 반짝이는 금속성 물체를 표현하기 위해서는 m_s를 크게 설정한다.



<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/c9145e65-b6f2-48ca-a3ff-70e15bc1358e" alt width="50%">
<em>2차원에서의 변환 결합 </em>
</center>

```c++
bool Renderer::isBackFace(int nFace)
{
	float normalZ = 0;
	Vector4 v1, v2, v3;
	v1.x = m_tramsformedVertex[m_face[nFace].m_vertex[0] - 1][0];
	v1.y = m_tramsformedVertex[m_face[nFace].m_vertex[0] - 1][1];
	v2.x = m_tramsformedVertex[m_face[nFace].m_vertex[1] - 1][0];
	v2.y = m_tramsformedVertex[m_face[nFace].m_vertex[1] - 1][1];
	v3.x = m_tramsformedVertex[m_face[nFace].m_vertex[2] - 1][0];
	v3.y = m_tramsformedVertex[m_face[nFace].m_vertex[2] - 1][1];
	normalZ = ((v2.x - v1.x) * (v3.y - v1.y)) - ((v2.y - v1.y) * (v3.x - v1.x));

	if (normalZ < 0) return true;
	return false;
}

```

```c++
g_renderer.readTextureFile("speckled213.raw");
g_renderer.makeUVVertex();
g_renderer.makefaceNormals();
g_renderer.makeVertexNormals();

void Renderer::readTextureFile(char* pFileName)
{
	FILE* input_file;

	char input_data[checkImageHeight][checkImageWidth][3];

	input_file = fopen(pFileName, "rb");

	fread(input_data, sizeof(char), checkImageWidth * checkImageHeight * 3, input_file);

	fclose(input_file);

	for (int i = 0; i < checkImageHeight; i++)
	{
		for (int j = 0; j < checkImageWidth; j++)
		{
			m_texture[i][j][0] = (GLubyte)input_data[i][j][0];
			m_texture[i][j][1] = (GLubyte)input_data[i][j][1];
			m_texture[i][j][2] = (GLubyte)input_data[i][j][2];
		}
	}
}

void Renderer::makeVertexUVs()
{
	for (int i = 0; i < m_nNumVertex; i++)
	{
		Vector4 v(m_vertex[i], 1);
		m_uv[i][0] = (v.x + 1) * 320;
		m_uv[i][1] = (v.y + 1) * 240;
	}
}

void Renderer ::makefaceNormals() //외적 
{
	for (int i = 0; i < m_nNumFace; i++) // 각 face의 normal
	{
		Vector4 v1, v2;

		v1.x = m_vertex[m_face[i].m_vertex[0] - 1][0] - m_vertex[m_face[i].m_vertex[1] - 1][0];
		v1.y = m_vertex[m_face[i].m_vertex[0] - 1][1] - m_vertex[m_face[i].m_vertex[1] - 1][1];

		v1.z = m_vertex[m_face[i].m_vertex[0] - 1][2] - m_vertex[m_face[i].m_vertex[1] - 1][2];

		v2.x = m_vertex[m_face[i].m_vertex[1] - 1][0] - m_vertex[m_face[i].m_vertex[2] - 1][0];
		v2.y = m_vertex[m_face[i].m_vertex[1] - 1][1] - m_vertex[m_face[i].m_vertex[2] - 1][1];

		v2.z = m_vertex[m_face[i].m_vertex[1] - 1][2] - m_vertex[m_face[i].m_vertex[2] - 1][2];

		Vector4 ret;

		ret = crossProduct(v1, v2);
		ret = normalize(ret);

		m_faceNormal[i][0] = ret.x;
		m_faceNormal[i][1] = ret.y;
		m_faceNormal[i][2] = ret.z;
	}

}

void Renderer::makeVertexNormals()
{
	for (int i = 0; i < m_nNumFace; i++) // //점의 노말은 점이 속한 삼각형 노말의 평균
	{
		for (int j = 0; j < 3; j++)
		{  
			m_vertexNormal[m_face[i].m_vertex[j] - 1][0] += m_faceNormal[i][0];
			m_vertexNormal[m_face[i].m_vertex[j] - 1][1] += m_faceNormal[i][1];
			m_vertexNormal[m_face[i].m_vertex[j] - 1][2] += m_faceNormal[i][2];
		}
	}

	for (int i = 0; i < m_nNumVertex; i++)
	{
		Vector4 v;

		v.x = m_vertexNormal[i][0];
		v.y = m_vertexNormal[i][1];
		v.z = m_vertexNormal[i][2];
		
		v = normalize(v);

		m_vertexNormal[i][0] = v.x;
		m_vertexNormal[i][1] = v.y;
		m_vertexNormal[i][2] = v.z;
	}

}
```




```c++
void Renderer::render()
{
	if (!texureMappingEnabled)
	{
		clearCheckImage();
		clearZBuffer();
		Matrix4 mvp = m_proj * m_view * m_world; //m_world 부터 역순으로 vector에 곱함
		applyMatrix(mvp, m_world);
		clearEdgetable();

		for (int i = 0; i < m_nNumFace; i++)
		{
			clearEdgetable();
			if (isCullEnabled)
			{
				if (!isBackFace(i))
				{
					buildEdgetable(i);
					fill(m_face[i].m_color);
				}
			}
			else
			{
				buildEdgetable(i);
				fill(m_face[i].m_color);
			}
		}
	}
}

void Renderer::buildEdgetable(int nFace)
{
	float vertices[2][3];
	int ymin;
	float yMax, x, inverseOfSlope, z, zPerY;
	
	for (int i = 0; i < m_face[nFace].m_nNumVertex; i++)
	{
		for (int j = 0; j < 3; j++)
		{
			vertices[0][j] = m_tramsformedVertex[m_face[nFace].m_vertex[i] - 1][j];
			vertices[1][j] = m_tramsformedVertex[m_face[nFace].m_vertex[(i + 1) % m_face[nFace].m_nNumVertex] - 1][j];
		}

		float uvVertices[2][2];
		float u, v, uPerY, vPerY;
		for (int j = 0; j < 2; j++)
		{
			uvVertices[0][j] = m_uv[m_face[nFace].m_vertex[i] - 1][j];
			uvVertices[1][j] = m_uv[m_face[nFace].m_vertex[(i + 1) % m_face[nFace].m_nNumVertex] - 1][j];
		}

		if (vertices[0][1] == vertices[1][1]) continue;
		else
		{
			inverseOfSlope = (vertices[1][0] - vertices[0][0]) / (vertices[1][1] - vertices[0][1]); //xperY
		}
		
		zPerY = (vertices[1][2] - vertices[0][2]) / (vertices[1][1] - vertices[0][1]);
		uPerY = (uvVertices[1][0] - uvVertices[0][0]) / (vertices[1][1] - vertices[0][1]); 
		vPerY = (uvVertices[1][1] - uvVertices[0][1]) / (vertices[1][1] - vertices[0][1]);

		float savedY;
		float savedV;

		if (vertices[0][1] < vertices[1][1]) // 작을때 ceiling 클 때 floor 
		{
			savedY = vertices[0][1];
			ymin = ceil(vertices[0][1]);
			ymin = max(ymin, 0);

			if (ymin > checkImageHeight - 1) continue;

			m_ET[ymin][m_indexCount[ymin]].x = vertices[0][0];

			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].x += (ymin - savedY) * inverseOfSlope;
			}

			m_ET[ymin][m_indexCount[ymin]].zperY = zPerY;
			m_ET[ymin][m_indexCount[ymin]].yMax = vertices[1][1];
			m_ET[ymin][m_indexCount[ymin]].inverseOfSlope = inverseOfSlope;
			m_ET[ymin][m_indexCount[ymin]].z = vertices[1][2];

			if (savedY != 0) v = ymin * uvVertices[0][1] / savedY;
			else v = 0;

			m_ET[ymin][m_indexCount[ymin]].u = uvVertices[0][0];
			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].u += (ymin - savedY) * uPerY;
			}
			m_ET[ymin][m_indexCount[ymin]].v = v;
			m_ET[ymin][m_indexCount[ymin]].uPerY = uPerY;
			m_ET[ymin][m_indexCount[ymin]].vperY = vPerY;
			m_indexCount[ymin]++;
		}
		else
		{
			savedY = vertices[1][1];
			ymin = ceil(vertices[1][1]);
			ymin = max(ymin, 0);

			if (ymin > checkImageHeight - 1) continue;
			m_ET[ymin][m_indexCount[ymin]].x = vertices[1][0];
			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].x += (ymin - savedY) * inverseOfSlope;
			}
			m_ET[ymin][m_indexCount[ymin]].yMax = vertices[0][1];  
			m_ET[ymin][m_indexCount[ymin]].inverseOfSlope = inverseOfSlope;
			m_ET[ymin][m_indexCount[ymin]].z = vertices[0][2];
			savedV = uvVertices[1][1];
			if (savedY != 0)
			{
				v = ymin * uvVertices[1][1] / savedY;
			}
			else v = 0;
			m_ET[ymin][m_indexCount[ymin]].u = uvVertices[1][0];
			if (ymin - savedY != 0)
			{
				m_ET[ymin][m_indexCount[ymin]].u += (ymin - savedY) * uPerY;
			}
			m_ET[ymin][m_indexCount[ymin]].v = v;
			m_ET[ymin][m_indexCount[ymin]].uPerY = uPerY;
			m_ET[ymin][m_indexCount[ymin]].vperY = vPerY;
			m_indexCount[ymin]++;
		}
	}
}

void Renderer::fill(GLubyte color[3])
{
	// AET
	for (int i = 0; i < checkImageHeight; i++)
	{

		//update intersection
		for (int j = 0; j < m_numEdgeInAET; j++)
		{
			m_AET[j].x += m_AET[j].inverseOfSlope;
			m_AET[j].z += m_AET[j].zperY;
			m_AET[j].u += m_AET[j].uPerY;
			m_AET[j].v += m_AET[j].vperY;
		}

		//Add new edge
		for (int j = 0; j < m_indexCount[i]; j++)
		{
			m_AET[m_numEdgeInAET + j] = m_ET[i][j];
		}
		m_numEdgeInAET += m_indexCount[i];

		//Delete edge
		for (int j = 0; j < m_numEdgeInAET; j++)
		{
			if (m_AET[j].yMax < i)
			{
				for (int k = j; k < m_numEdgeInAET; k++)
				{
					m_AET[k] = m_AET[k + 1];
				}
				j--;
				m_numEdgeInAET--;
			}
		}

		//Sort intersections
		Edge temp;
		for (int j = 0; j < m_numEdgeInAET - 1; j++)
		{
			for (int k = j + 1; k < m_numEdgeInAET; k++)
			{
				if (m_AET[j].x > m_AET[k].x)
				{
					temp = m_AET[j];
					m_AET[j] = m_AET[k];
					m_AET[k] = temp;
				}
			}
		}

		//Render
		for (int j = 0; j < m_numEdgeInAET; j += 2)
		{
			int k;
			int xmin = floor(m_AET[j].x);
			int xmax = floor(m_AET[j + 1].x);
			xmin = max(xmin, 0);
			xmax = min(xmax, checkImageWidth - 1);
			float uPerX = (m_AET[j + 1].u - m_AET[j].u) / (xmax - xmin);
			float deltaX = 0;
			float vPerX = (m_AET[j + 1].v - m_AET[j].v) / (xmax - xmin);
			float deltaV = 0;
			float zPerX = (m_AET[j + 1].z - m_AET[j].z) / (xmax - xmin);
			float deltaZ = 0;

			for (k = xmin; k < xmax; k++)
			{
				if (m_AET[j].z + deltaZ < zBuffer[i][k])
				{
					checkImage[i][k][0] = (GLubyte)texture[(int)(m_AET[j].v + deltaV)][(int)(m_AET[j].u + deltaX)][0];
					checkImage[i][k][1] = (GLubyte)texture[(int)(m_AET[j].v + deltaV)][(int)(m_AET[j].u + deltaX)][1];
					checkImage[i][k][2] = (GLubyte)texture[(int)(m_AET[j].v + deltaV)][(int)(m_AET[j].u + deltaX)][2];

					zBuffer[i][k] = m_AET[j].z + deltaZ;
					deltaX += uPerX;
					deltaV += vPerX;
					deltaZ += zPerX;
				}
			}
		}
	}
}

```




<center><img src="https://github.com/limbsoo/limbsoo.github.io/assets/96706760/c9145e65-b6f2-48ca-a3ff-70e15bc1358e" alt width="50%">
<em>2차원에서의 변환 결합 </em>
</center>